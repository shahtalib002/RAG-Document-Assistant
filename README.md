## ğŸ§  Overview

The **RAG Document Assistant** is an AI-powered web application that allows users to **chat with their documents** â€” such as PDFs, DOCX, and TXT files.  
It uses the **Retrieval-Augmented Generation (RAG)** technique to find relevant text from documents and answer user questions accurately using **Groq-hosted Large Language Models (LLMs)**.

This project combines the power of:
- ğŸ§© **LangChain** for building the RAG pipeline  
- âš¡ **Groq LLMs** for fast and intelligent text generation  
- ğŸ“š **FAISS** for local vector search  
- ğŸ–¥ï¸ **Streamlit** for an easy-to-use web interface  

---

## ğŸ¯ Features

âœ… Upload multiple files (PDF, DOCX, TXT)  
âœ… Ask questions and get precise, context-aware answers  
âœ… Stores document embeddings locally using **FAISS**  
âœ… Conversational memory â€” remembers previous queries  
âœ… Provides cited document sources for transparency  
âœ… Fully customizable (chunk size, overlap, top-k, temperature)  
âœ… Safe API key management using `.env` or `secrets.toml`  
âœ… Works offline for retrieval once embeddings are created  

---
## âš™ï¸ Tech Stack

| Component | Technology Used |
|------------|----------------|
| Frontend | Streamlit |
| LLM Framework | LangChain |
| LLM Provider | Groq (via `langchain_groq`) |
| Embeddings | Sentence Transformers (`all-MiniLM-L6-v2`) |
| Vector Database | FAISS |
| Document Parsing | PyPDF2, docx2txt |
| Environment Management | python-dotenv |
| Programming Language | Python 3.10 |

---

## ğŸ§© How It Works

1. **File Upload** â†’ You upload one or more documents.  
2. **Text Extraction** â†’ Text is extracted from each document (PDF/DOCX/TXT).  
3. **Chunking** â†’ The text is split into smaller overlapping chunks (default: 500 chars).  
4. **Embedding** â†’ Each chunk is converted into a numeric vector using `sentence-transformers`.  
5. **Storage** â†’ All vectors are stored locally in a FAISS index.  
6. **Retrieval** â†’ When you ask a question, the app retrieves the most relevant chunks.  
7. **Generation** â†’ The question + retrieved text are passed to a Groq LLM (e.g., `llama-3.3-70b-versatile`) for generating an answer.  
8. **Display** â†’ The final answer and source documents are shown in the Streamlit interface.

---

## ğŸ§  Example Use Cases

| Scenario | Example Query | Example Response |
|-----------|----------------|------------------|
| Resume Analysis | â€œWhat type of job can this candidate apply for?â€ | Suggests AI/ML Engineer, Data Scientist, etc. |
| Research Paper Review | â€œWhat are the main contributions of this paper?â€ | Generates concise summary |
| Legal Document | â€œWhat are the key clauses in this agreement?â€ | Lists major clauses and conditions |
| Business Reports | â€œSummarize financial performance.â€ | Generates short analysis summary |

---

## ğŸš€ Getting Started

### 1ï¸âƒ£ Clone the Repository
```bash
git clone https://github.com/shahtalib002/RAG-Document-Assistant.git
cd RAG-Document-Assistant
